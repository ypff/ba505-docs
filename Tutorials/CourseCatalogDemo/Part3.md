# Part 3: Creating a Web Spider with Scrapy
## Issues Covered
* How spiders work to scrape data from multiple linked pages
* How code generators can create framework 'boilerplate' code

## Practices Demonstrated
* Generating, configuring, and running Scrapy spiders
* Writing new Python classes for use with generated framework code
* Writing and running a bash script from the command line

## Creating a Scrapy Project
__1. Learn about Scrapy.__  
[Scrapy](https://scrapy.org/) is a third-party library for creating and running *web spiders*. A web spider is a program that can browse whole websites, page by page, by following links. On each page the spider can collect data for processing later. The most famous web spider today is used by Google to discover and catalog the web for search. 
__2. Get ready to do some work.__   
Start JupyterLab and launch a new Terminal.   

Fork the project from GitHub Classroom:
https://classroom.github.com/a/qpM6KZsu
Then clone it to your JupyterLab home folder.  

__3. Install Scrapy.__  
**If you are using the class JupterHub then you can skip this step (but read on anway).** If you are using Anaconda to run Jupyter on your desktop, then you will need to install Scrapy before you can use it. In the terminal type the following command:
```bash
pip install scrapy
```
`pip` is a package manager for Python. It's job is to install things from the 'usual' places. It is the equivalent of the app store for Macs or Google Play for android phones or ChromeOS. If the package you want to install requires other things to work, then `pip` install those too. It also warns you of potential incompatibilities, where two packages expect (and depend on) different versions of a third package. 
 
__4. Create a new Scrapy project.__  

In the Terminal, use the `cd` command to navigate to your new repository folder. [From here on we are going to work from the instructions given in the [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html). Open it so you you can compare. We will be making a few minor tweaks to suit our needs.]

The following will create a new Scrapy project folder:
```bash
scrapy startproject catalog
```  
Scrapy will confirm with a few lines of explanatory text with the project name, file path, and instructions for creating your first spider. 

## Writing the Required Framework Classes
__1. Study the generated code.__  
Take a moment to look through the many files and folders generated by Scrapy. (You can do that directly in JupyterLab without using the Terminal.) You'll see a few files and one folder called `spiders`. These are framework 'boilerplate' for a spider app that we can run from the Terminal. We won't need to do much from here, just add one file and edit another.   
__2. Configure metadata for scraped data.__  
Open the `catalog/catalog/items.py` file for editing. You should see something like this:  
```python
# -*- coding: utf-8 -*-
    
# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy

class CatalogItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass
```
    
This is your first class definition, with Scrapy providing most of the code for you. The `CatalogItem` class is a custom data type that we'll use to define the columns (fields) of the table we want to construct. While not strictly necessary, creating a custom data type like this can make our code easier to test and debug. 

From the docs:
> The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Scrapy spiders can return the extracted data as Python dicts. While convenient and familiar, Python dicts lack structure: it is easy to make a typo in a field name or return inconsistent data, especially in a larger project with many spiders.

For more about scrapy `Item` classes, go to https://doc.scrapy.org/en/latest/topics/items.html. In our case, each `CatalogItem` is a course in the online catalog. 

Follow the `name = scrapy.Field()` pattern given in the comments to add the following fields (just above the `pass` statement):
- catalogid
- title
- credits
- attributes
- prereqs
- coreqs
- fees
- description   
    
When you are done delete the `pass` statement that was acting as a placeholder for your new code.  
How did you do? The finished code should look like this:
```python
# -*- coding: utf-8 -*-
    
# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html
    
import scrapy
    
class CatalogItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    catalogid = scrapy.Field()
    title = scrapy.Field()
    credits = scrapy.Field()
    attributes = scrapy.Field()
    coreqs = scrapy.Field()
    prereqs = scrapy.Field()
    fees = scrapy.Field()
    description = scrapy.Field()
```
Don't forget to save the file after editing.

__3. Add the spider code used to scrape data.__   
Now we need to create the spider that will browse the website and scrape out CatalogItems for us. For that we'll need to delve down into the `spiders` folder, which is currently empty except for a blank `__init__.py` file.

Add a new file called `catalog_spider.py` to the `spiders` folder. Within the file, follow the pattern given in the [Scrapy tutorial](https://doc.scrapy.org/en/latest/intro/tutorial.html#our-first-spider) to create a new class called `CatalogSpider`:

```python
import scrapy
    
class CatalogSpider(scrapy.Spider):
        name = "catalog"
    
        def start_requests(self):
            urls = ['http://catalog.fairfield.edu/courses/']
            for url in urls:
                yield scrapy.Request(url=url, callback=self.parse)
    
        def parse(self, response):
            links = response.css('a::attr(href)').re(r'/courses/.+')
            
            for link in links:
                yield response.follow(url=link, callback=self.parse_program)
        
        def parse_program(self,response):
            program_code =response.css('.page-title::text').re_first(r'\(\w+\)').strip('()')
            program_name =response.css('.page-title::text').extract_first().split(' (')[0]
            extratypes = {'Attributes: ':'attributes','Attribute: ':'attributes',
                'Fee:':'fees',
                'Corequisite: ':'coreqs','Corequisites: ':'coreqs',
                'Prerequisite: ':'prereqs','Prerequisites: ':'prereqs'}
            courseblocks = response.css('.courseblock')
            for courseblock in courseblocks:
                titleblock = courseblock.css('.courseblocktitle strong::text').extract_first()
                catalogid = titleblock.split('\xa0')[0]
                coursetitle = titleblock.split('\xa0')[1]
                credits = courseblock.css('.courseblocktitle .credits::text').extract_first()
                extratexts = courseblock.css('.courseblockextra *::text').extract()
                extras = {'attributes':'','fees':'','coreqs':'','prereqs':''}

                typ=""
                for etxt in extratexts:
                    if etxt in extratypes:
                        if typ != "":
                            extras[extratypes[typ]] = txt.replace('\xa0',' ').strip()
                        typ =etxt
                        txt =""
                    else:
                        txt += etxt
                if typ != "":
                    extras[extratypes[typ]] = txt.replace('\xa0',' ').strip()
                
                description = courseblock.css('.courseblockdesc::text').extract_first()
                yield {
                    'program_code':program_code,
                    'program_name':program_name,
                    'catalog_id':catalogid,
                    'course_title':coursetitle,
                    'credits':credits,
                    'prereqs':extras['prereqs'],
                    'coreqs':extras['coreqs'],
                    'fees':extras['fees'],
                    'attributes':extras['attributes'],
                    'description':description
                }
```
Let's take each of the methods one by one to understand what they are supposed to do. It's okay if you don't get the code exactly. You can come back later if you need to create a spider of your own. Instead, focus on the concepts illustrated here. 

The `.start_requests()` method starts the process by crawling any pages in the `urls` list. In this case we are starting with http://catalog.fairfield.edu/courses, which links to course descriptions for every course in the catalog. For each of our starter URls, we use a [yield statement](https://docs.python.org/3/howto/functional.html#generators) to generate a `Request` object that i) visits the URL and then ii) calls the `.parse()` method with the resulting HTML. In other words, the method parses the http://catalog.fairfield.edu/courses/ page and returns any  `CatalogItem` data it scraped along the way.  

The `.parse()` method is responsible for reading the HTML on a page (as given by the `response` parameter). In our case, the method i) makes a list of all the links found on the page and then ii) follows each link, one by one, to generate (yield) `CatalogItem` data. The course descriptions are organized, one page per academic area (program). For example, the http://catalog.fairfield.edu/courses/ac page lists every accounting course in the catalog. 

Now for the meat of the class: the `.parse_program()` method that will do the actual scraping of the `CatalogItem` data for a given academic program page. It is called at the very end of the `.parse()` method. To fully make sense of this code you will need to know something about HTML and CSS. However, we can infer a lot with a few educated guesses. **Take a read through and see if you explain it to your tablemates.**
    
## Running/Testing and Deployment
__1. Running from a `bash` command line__  

To run the spider type the following from within the `catalog` folder.
```bash
scrapy crawl catalog -o catalog.csv
```   
        
Of course, you could have a bug. Did you find a `catalog.csv` file? If so, open it. Does it have what you expect? Compare what's in the file with the online catalog. If not, you may have a bug in your code. Fix it.  
    
__2. Running within a `bash` script__  
We can run bash commands from a file and even execute the script from the terminal. Create a new file named `catalogcrawl.sh` in the root of this repository. Then paste in the following bash code:
```bash
cd catalog; scrapy crawl catalog -o catalog.csv
```
The first part (`cd catalog;`) drops the script down into the catalog folder before trying to run the spider. 
    To run the shell script in a terminal type:
```bash
sh catalogcrawl.sh
```       
We're almost done. All we have to do is make our new shell script executable as a new bash command. ([RTFM](http://www.tldp.org/LDP/abs/html/abs-guide.html#SHA-BANG).)  
Inside the script file add a "hashbang" line to the top so it looks like:
```bash
#!/bin/bash
cd catalog; scrapy crawl catalog -o catalog.csv
```
The hashbang tells the terminal that the script is written in bash, giving the absolute file path to the bash executable. 

Next, in the terminal give the bash permission to execute the file as follows:
```bash
chmod +x catalogcrawl.sh
```
Running the script is then as easy as typing the following in the terminal window. 
```bash
./catalogcrawl.sh
```
Wanna run the spider every night at, say 2am, to refresh the data? Use `cron` to schedule a call our new shell script. That's well beyong the scope of this class, so [RTFM](https://awc.com.my/uploadnew/5ffbd639c5e6eccea359cb1453a02bed_Setting%20Up%20Cron%20Job%20Using%20crontab.pdf) on your own for that. 

__3. Deploying to Production Environments__  
Scrapy supports a couple of canned servers for production environments: 
  - [Scrapyd](https://doc.scrapy.org/en/latest/topics/deploy.html#deploy-scrapyd) (open source)
  - [Scrapy Cloud](https://doc.scrapy.org/en/latest/topics/deploy.html#deploy-scrapy-cloud) (cloud-based)  
  
Follow the instructions to install one of these if needed.
